import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
from datetime import datetime
import os
import time
import random
import logging
from dataclasses import dataclass, asdict

# Configuration classes moved from config.py
class PathConfig:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹é–¢é€£ã®è¨­å®š"""
    # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸå®¶è³ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    OUTPUT_FILE = 'data/processed/rent_marketprice.json'
    # éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    PREFECTURE_CODE_MAP_FILE = 'backend/app/const/prefecture_codes.json'

class ScrapingConfig:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³å¯¾è±¡ã‚µã‚¤ãƒˆã‚„ãƒ˜ãƒƒãƒ€ãƒ¼ã«é–¢ã™ã‚‹è¨­å®š"""
    BASE_URL = "https://suumo.jp/chintai/soba/"
    USER_AGENTS = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
        'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:89.0) Gecko/20100101 Firefox/89.0'
    ]
    HEADERS = {
        'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
    }

class RequestConfig:
    """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚„ãƒªãƒˆãƒ©ã‚¤ã«é–¢ã™ã‚‹è¨­å®š"""
    MAX_RETRIES = 3                 # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°
    RETRY_DELAY_SECONDS = 5         # ãƒªãƒˆãƒ©ã‚¤æ™‚ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    REQUEST_MIN_DELAY = 1.0         # å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã®æœ€å°å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    REQUEST_MAX_DELAY = 3.0         # å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‰ã®æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
    REQUEST_TIMEOUT_SECONDS = 15    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ï¼ˆç§’ï¼‰

class ParallelConfig:
    """ä¸¦åˆ—å‡¦ç†ã«é–¢ã™ã‚‹è¨­å®š"""
    MAX_WORKERS = 10                # ä¸¦åˆ—å‡¦ç†ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¹ãƒ¬ãƒƒãƒ‰æ•° 

# logging ã®åŸºæœ¬è¨­å®š
# INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã€ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®å½¢å¼ã§è¡¨ç¤º
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«å®šç¾© ---
@dataclass(frozen=True)
class Prefecture:
    """éƒ½é“åºœçœŒæƒ…å ±ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    url: str

@dataclass(frozen=True)
class Line:
    """è·¯ç·šæƒ…å ±ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    url: str

@dataclass(frozen=True)
class StationRentInfo:
    """åé›†ã—ãŸç”Ÿã®é§…å®¶è³ƒæƒ…å ±ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    station: str
    rent: str

@dataclass
class StationData:
    """å‡¦ç†æ¸ˆã¿ã®é§…å®¶è³ƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    prefecture: str
    railway_company: str
    line_name: str
    station_name: str
    rent: float
    lastupdate: str | None = None


class SuumoScraper:
    """SUUMOã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‹ã‚‰å®¶è³ƒç›¸å ´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""
    def __init__(self, headers: dict):
        self.headers = headers

    def make_request_with_retry(self, url: str, error_context: str) -> requests.Response | None:
        """æŒ‡å®šã•ã‚ŒãŸURLã«å¯¾ã—ã¦ãƒªãƒˆãƒ©ã‚¤ä»˜ãã®GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹"""
        for attempt in range(RequestConfig.MAX_RETRIES):
            time.sleep(random.uniform(RequestConfig.REQUEST_MIN_DELAY, RequestConfig.REQUEST_MAX_DELAY))
            request_specific_headers = self.headers.copy()
            if ScrapingConfig.USER_AGENTS:
                request_specific_headers["User-Agent"] = random.choice(ScrapingConfig.USER_AGENTS)
            
            try:
                response = requests.get(url, headers=request_specific_headers, timeout=RequestConfig.REQUEST_TIMEOUT_SECONDS)
                response.raise_for_status()
                response.encoding = response.apparent_encoding
                return response
            except requests.RequestException as e:
                logger.warning(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ ({error_context}, è©¦è¡Œ {attempt + 1}/{RequestConfig.MAX_RETRIES}): {e} ({url})")
                if attempt + 1 == RequestConfig.MAX_RETRIES:
                    logger.error(f"ğŸš¨ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸã€‚{error_context} ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {url}")
                    return None
                time.sleep(RequestConfig.RETRY_DELAY_SECONDS)
        return None

    def get_prefecture_info_list(self, base_url: str) -> list[Prefecture]:
        """éƒ½é“åºœçœŒåã¨è·¯ç·šä¸€è¦§ãƒšãƒ¼ã‚¸URLã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹"""
        response = self.make_request_with_retry(base_url, "éƒ½é“åºœçœŒãƒªã‚¹ãƒˆå–å¾—")
        if not response:
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        prefecture_links = soup.find_all("a", class_="areamenu_detail-btn")

        prefecture_info_list = []
        for tag in prefecture_links:
            href = tag.get("href")
            name = tag.text.strip()
            if href:
                full_url = urljoin(base_url, href)
                full_url = f"{full_url}ensen/" 
                prefecture_info_list.append(Prefecture(name=name, url=full_url))
                
        return prefecture_info_list

    def get_line_info_list(self, base_url: str) -> dict[str, list[Line]]:
        """æŒ‡å®šã•ã‚ŒãŸéƒ½é“åºœçœŒã®è·¯ç·šæƒ…å ±ã‚’å–å¾—ã™ã‚‹"""
        response = self.make_request_with_retry(base_url, "è·¯ç·šãƒªã‚¹ãƒˆå–å¾—")
        if not response:
            return {}

        soup = BeautifulSoup(response.text, "html.parser")
        search_table = soup.find("table", class_="searchtable")
        if not search_table:
            logger.info(f"ğŸ’¡ æ³¨æ„: è·¯ç·šæƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ« (searchtable) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ ({base_url})")
            return {} 

        search_table_list = search_table.find_all("tr")
        company_to_lines_map = {}

        for railway_company_row in search_table_list:
            railway_company_th = railway_company_row.find("th" , class_="searchtable-title")
            if not railway_company_th:
                continue
            railway_company_name = railway_company_th.text.strip()
            company_to_lines_map[railway_company_name] = []

            for searchitem_td in railway_company_th.find_next_siblings("td"):
                line_links = searchitem_td.find_all("a")
                for link in line_links:
                    href = link.get("href")
                    line_name = link.text.strip()
                    if href:
                        full_url = urljoin(base_url, href)
                        company_to_lines_map[railway_company_name].append(Line(name=line_name, url=full_url))
                    else:
                        logger.info(f"ğŸ’¡ æ³¨æ„: è·¯ç·šãƒªãƒ³ã‚¯ã® href ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {line_name} ({base_url})")
        
        return company_to_lines_map
    
    def get_station_rent_list(self, line_url: str) -> list[StationRentInfo]:
        """æŒ‡å®šã•ã‚ŒãŸè·¯ç·šã®é§…ã”ã¨ã®å®¶è³ƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹"""
        response = self.make_request_with_retry(line_url, "é§…å®¶è³ƒãƒ‡ãƒ¼ã‚¿å–å¾—")
        if not response:
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        station_rows = soup.find_all("tr", class_="js-graph-data")
        results = []

        if not station_rows:
            logger.info(f"ğŸ’¡ æ³¨æ„: é§…ãƒ‡ãƒ¼ã‚¿è¡Œ (js-graph-data) ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ ({line_url})")
            return []

        for row in station_rows:
            first_td_tag = row.find("td")
            station_name = first_td_tag.text.strip() if first_td_tag else "ä¸æ˜"
            
            price_tag_element = row.find("span", class_="graphpanel_matrix-td_graphinfo-strong")
            price_text = price_tag_element.text.strip() if price_tag_element else "ä¸æ˜"
            
            results.append(StationRentInfo(station=station_name, rent=price_text))
        
        return results

class ScrapingOrchestrator:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†å…¨ä½“ã‚’ç®¡ç†ã—ã€ä¸¦åˆ—å®Ÿè¡Œã‚’åˆ¶å¾¡ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    def __init__(self, scraper: SuumoScraper, max_workers: int):
        self.scraper = scraper
        self.max_workers = max_workers

    def execute(self, base_url: str) -> tuple[list[StationData], dict]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã€åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã¨çµ±è¨ˆæƒ…å ±ã‚’è¿”ã™"""
        prefectures = self.scraper.get_prefecture_info_list(base_url)
        prefecture_total_count = len(prefectures)

        if not prefectures:
            logger.error("éƒ½é“åºœçœŒãƒªã‚¹ãƒˆãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            stats = {"prefecture_total": 0, "processed_prefectures_with_lines": 0, "station_data_count": 0}
            return [], stats

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_station_data_result, processed_prefectures_with_lines = self._submit_line_tasks(executor, prefectures)
            all_station_data = self._collect_station_data(future_to_station_data_result)
        
        stats = {
            "prefecture_total": prefecture_total_count,
            "processed_prefectures_with_lines": processed_prefectures_with_lines,
            "station_data_count": len(all_station_data)
        }
        return all_station_data, stats

    def _submit_line_tasks(self, executor: ThreadPoolExecutor, prefectures: list[Prefecture]) -> tuple[dict, int]:
        future_to_lines_result = {
            executor.submit(self.scraper.get_line_info_list, pref.url): pref
            for pref in prefectures
        }

        future_to_station_data_result = {}
        processed_prefectures_count = 0

        for future in as_completed(future_to_lines_result):
            prefecture = future_to_lines_result[future]
            try:
                company_to_lines_map_result = future.result()
                if company_to_lines_map_result:
                    processed_prefectures_count += 1
                    for railway_company, lines in company_to_lines_map_result.items():
                        for line in lines:
                            future_to_station_data_result[executor.submit(self.scraper.get_station_rent_list, line.url)] = \
                                (prefecture.name, railway_company, line.name, line.url)
                else:
                    logger.info(f"ğŸ’¡ æ³¨æ„: {prefecture.name} ({prefecture.url}) ã®è·¯ç·šä¸€è¦§ãŒç©ºã§ã—ãŸï¼ˆãƒ‡ãƒ¼ã‚¿ãªã—ã¨ã—ã¦å‡¦ç†ï¼‰ã€‚")
            except Exception as exc:
                logger.error(f'{prefecture.name} ({prefecture.url}) ã®è·¯ç·šä¸€è¦§å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {exc}')
        return future_to_station_data_result, processed_prefectures_count

    def _collect_station_data(self, future_to_station_data_result: dict) -> list[StationData]:
        all_station_data = []
        for future in as_completed(future_to_station_data_result):
            pref_name, railway_company, line_name, line_url = future_to_station_data_result[future]
            try:
                station_data_list = future.result()
                if station_data_list:
                    for station_info in station_data_list:
                        station_name = station_info.station
                        rent_str = station_info.rent
                        
                        if station_name and rent_str and rent_str not in ["ä¸æ˜", "---"]: 
                            try:
                                rent_float = float(rent_str)
                                all_station_data.append(StationData(
                                    prefecture=pref_name,
                                    railway_company=railway_company,
                                    line_name=line_name,
                                    station_name=station_name,
                                    rent=rent_float
                                ))
                            except (ValueError, TypeError):
                                logger.warning(f"âš ï¸ å®¶è³ƒãƒ‡ãƒ¼ã‚¿ '{rent_str}' ã‚’æ•°å€¤ã«å¤‰æ›ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚({pref_name}, {line_name}, {station_name})")
                else:
                    logger.info(f"ğŸ’¡ {pref_name} ã® {line_name} ({line_url}) ã®é§…ãƒ‡ãƒ¼ã‚¿ã¯ç©ºã§ã—ãŸã€‚")
            except Exception as exc:
                logger.error(f'é§…å®¶è³ƒãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({pref_name}, {line_name}): {exc}', exc_info=True)
                
        return all_station_data

def load_prefecture_code_map(file_path: str) -> dict:
    """éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return {}
    except json.JSONDecodeError:
        logger.error(f"ã‚¨ãƒ©ãƒ¼: éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®JSONå½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {file_path}")
        return {}


def sort_data_by_prefecture_code(station_data_list: list[StationData], prefecture_codes_map: dict) -> list[StationData]:
    """éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦é§…ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆã™ã‚‹"""
    if not prefecture_codes_map:
        logger.warning("éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãŒç©ºã®ãŸã‚ã€ã‚½ãƒ¼ãƒˆå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return station_data_list

    def get_sort_key(item: StationData) -> int | float:
        """ã‚½ãƒ¼ãƒˆç”¨ã®ã‚­ãƒ¼ã‚’è¿”ã™ï¼ˆéƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ç„¡é™å¤§ï¼‰"""
        # ãƒãƒƒãƒ—ã‹ã‚‰éƒ½é“åºœçœŒã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆæ–‡å­—åˆ—ï¼‰ã€‚è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°None
        code_str = prefecture_codes_map.get(item.prefecture)
        # ã‚³ãƒ¼ãƒ‰ã‚’æ•´æ•°ã«å¤‰æ›ã—ã‚ˆã†ã¨è©¦ã¿ã‚‹ã€‚å¤‰æ›ã§ããªã„ã€ã¾ãŸã¯Noneã®å ´åˆã¯float('inf')ã‚’è¿”ã™
        try:
            return int(code_str) if code_str is not None else float('inf')
        except (ValueError, TypeError):
            return float('inf')

    # ã‚½ãƒ¼ãƒˆã‚­ãƒ¼é–¢æ•°ã‚’ä½¿ã£ã¦ãƒªã‚¹ãƒˆã‚’ã‚½ãƒ¼ãƒˆ
    sorted_list = sorted(station_data_list, key=get_sort_key)
    
    # ã‚½ãƒ¼ãƒˆã§ããªã‹ã£ãŸé …ç›®ï¼ˆã‚³ãƒ¼ãƒ‰ä¸æ˜ï¼‰ãŒã‚ã‚Œã°ãƒ­ã‚°ã§é€šçŸ¥
    not_sorted_items = [item.prefecture for item in sorted_list if get_sort_key(item) == float('inf')]
    if not_sorted_items:
        logger.info(f"ğŸ’¡ æ³¨æ„: æ¬¡ã®éƒ½é“åºœçœŒã¯ã‚³ãƒ¼ãƒ‰ãŒä¸æ˜ãªãŸã‚ã€ã‚½ãƒ¼ãƒˆé †ãŒä¿è¨¼ã•ã‚Œã¾ã›ã‚“: {list(set(not_sorted_items))}")
        
    return sorted_list


def save_data_to_json(station_data_list: list[StationData], filename: str):
    """åé›†ãƒ»å‡¦ç†ã—ãŸé§…ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
    output_dir = os.path.dirname(filename)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)
        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {output_dir}")

    # StationDataã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾æ›¸ã®ãƒªã‚¹ãƒˆã«å¤‰æ›
    data_to_save = [asdict(data) for data in station_data_list]

    # ç¾åœ¨æ™‚åˆ»ã‚’'lastupdate'ã‚­ãƒ¼ã¨ã—ã¦å„è¾æ›¸ã«è¿½åŠ 
    current_time_iso = datetime.now().isoformat()
    for item in data_to_save:
        item['lastupdate'] = current_time_iso
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=4)
        logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {filename} ({len(data_to_save)}ä»¶)")
    except IOError as e:
        logger.error(f"ğŸš¨ ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

def main():
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    start_time = time.time()
    
    scraper = SuumoScraper(headers=ScrapingConfig.HEADERS)
    orchestrator = ScrapingOrchestrator(scraper=scraper, max_workers=ParallelConfig.MAX_WORKERS)
    
    all_station_data, stats = orchestrator.execute(base_url=ScrapingConfig.BASE_URL)
    
    prefecture_code_map = load_prefecture_code_map(PathConfig.PREFECTURE_CODE_MAP_FILE)
    sorted_station_data = sort_data_by_prefecture_code(all_station_data, prefecture_code_map)
    
    save_data_to_json(sorted_station_data, PathConfig.OUTPUT_FILE)
    
    end_time = time.time()
    logger.info(f"ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    logger.info(f"ğŸ“Š --- çµ±è¨ˆæƒ…å ± ---")
    logger.info(f"   - å…¨éƒ½é“åºœçœŒæ•°: {stats['prefecture_total']}")
    logger.info(f"   - ãƒ‡ãƒ¼ã‚¿å–å¾—å¯¾è±¡ã®éƒ½é“åºœçœŒæ•°: {stats['processed_prefectures_with_lines']}")
    logger.info(f"   - åé›†ã—ãŸé§…ãƒ‡ãƒ¼ã‚¿ç·æ•°: {stats['station_data_count']}")
    logger.info(f"   - å‡¦ç†æ™‚é–“: {end_time - start_time:.2f}ç§’")

if __name__ == '__main__':
    main() 